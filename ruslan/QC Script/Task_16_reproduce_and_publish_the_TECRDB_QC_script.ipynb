{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX8E1Mkij485",
        "outputId": "09e80646-6518-420d-9cbb-9104ff7dff91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-17 18:59:21--  https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=2123069643\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.128.100, 74.125.128.113, 74.125.128.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.128.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=2123069643 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-01-17 18:59:21--  https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=2123069643\n",
            "Resolving doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)... 173.194.69.132, 2a00:1450:4013:c04::84\n",
            "Connecting to doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘openTECR recuration - actual data.csv’\n",
            "\n",
            "openTECR recuration     [ <=>                ]   1.22M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-17 18:59:22 (9.33 MB/s) - ‘openTECR recuration - actual data.csv’ saved [1276759]\n",
            "\n",
            "--2025-01-17 18:59:22--  https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=831893235\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.128.100, 74.125.128.113, 74.125.128.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.128.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=831893235 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-01-17 18:59:22--  https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=831893235\n",
            "Resolving doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)... 173.194.69.132, 2a00:1450:4013:c04::84\n",
            "Connecting to doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘openTECR recuration - table codes.csv’\n",
            "\n",
            "openTECR recuration     [ <=>                ]  48.61K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-01-17 18:59:23 (5.64 MB/s) - ‘openTECR recuration - table codes.csv’ saved [49780]\n",
            "\n",
            "--2025-01-17 18:59:23--  https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=1475422539\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.128.100, 74.125.128.113, 74.125.128.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.128.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=1475422539 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-01-17 18:59:23--  https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=csv&gid=1475422539\n",
            "Resolving doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)... 173.194.69.132, 2a00:1450:4013:c04::84\n",
            "Connecting to doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘openTECR recuration - table metadata.csv’\n",
            "\n",
            "openTECR recuration     [ <=>                ] 211.38K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-01-17 18:59:24 (5.36 MB/s) - ‘openTECR recuration - table metadata.csv’ saved [216452]\n",
            "\n",
            "--2025-01-17 18:59:24--  https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=ods\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.128.100, 74.125.128.113, 74.125.128.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.128.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=ods [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2025-01-17 18:59:24--  https://doc-00-8k-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/73nbh4anccgrg7oplu9esnjpjo/1737140360000/115120384215235060766/*/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c?format=ods\n",
            "Resolving doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)... 173.194.69.132, 2a00:1450:4013:c04::84\n",
            "Connecting to doc-00-8k-sheets.googleusercontent.com (doc-00-8k-sheets.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/vnd.oasis.opendocument.spreadsheet]\n",
            "Saving to: ‘openTECR recuration.ods’\n",
            "\n",
            "openTECR recuration     [    <=>             ]   2.43M  3.88MB/s    in 0.6s    \n",
            "\n",
            "2025-01-17 18:59:43 (3.88 MB/s) - ‘openTECR recuration.ods’ saved [2548477]\n",
            "\n",
            "--2025-01-17 18:59:43--  https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv\n",
            "Resolving w3id.org (w3id.org)... 162.209.11.63, 2001:4801:7820:75:be76:4eff:fe10:16a2\n",
            "Connecting to w3id.org (w3id.org)|162.209.11.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://zenodo.org/record/5495826/files/TECRDB.csv [following]\n",
            "--2025-01-17 18:59:44--  https://zenodo.org/record/5495826/files/TECRDB.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.48.194, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5495826/files/TECRDB.csv [following]\n",
            "--2025-01-17 18:59:44--  https://zenodo.org/records/5495826/files/TECRDB.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1662362 (1.6M) [text/plain]\n",
            "Saving to: ‘TECRDB.csv’\n",
            "\n",
            "TECRDB.csv          100%[===================>]   1.58M  10.4MB/s    in 0.2s    \n",
            "\n",
            "2025-01-17 18:59:44 (10.4 MB/s) - ‘TECRDB.csv’ saved [1662362/1662362]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"openTECR recuration - actual data.csv\" \"https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=2123069643\"\n",
        "!wget -O \"openTECR recuration - table codes.csv\" \"https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=831893235\"\n",
        "!wget -O \"openTECR recuration - table metadata.csv\" \"https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=csv&gid=1475422539\"\n",
        "!wget -O \"openTECR recuration.ods\" \"https://docs.google.com/spreadsheets/d/1jLIxEXVzE2SAzIB0UxBfcFoHrzjzf9euB6ART2VDE8c/export?format=ods\"\n",
        "!wget -O \"TECRDB.csv\" \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv\"\n",
        "##Downloading openTECR and Noor data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy\n",
        "## Installing library for processing .ods file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3iv5JwRk1Sk",
        "outputId": "b0d47e0f-45e2-4471-f921-a7e62a8575f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/717.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from odfpy) (0.7.1)\n",
            "Building wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160672 sha256=21887d614af743c8b1238dc15d7e101e0de368e5d08284e9c144872a5f072e2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/1d/c8/8c29be1d73ca42d15977c75193d9f39a98499413c2838ac54c\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy\n",
            "Successfully installed odfpy-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "READ_CSV = True\n",
        "\n",
        "\n",
        "## data upload\n",
        "if READ_CSV:\n",
        "    df = pandas.read_csv(\"openTECR recuration - actual data.csv\")\n",
        "else:\n",
        "    df = pandas.read_excel(\"openTECR recuration.ods\", sheet_name=\"actual data\")\n",
        "df = df.replace({\"col l/r\": {\"l\":1,\"r\":2}}) ##replaced l and r in col l/r with 1 and 2\n",
        "\n",
        "## containing NaNs\n",
        "print(\"containing NaNs:\")\n",
        "\n",
        "#script that counts rows containing NaN values\n",
        "subset = [\"part\", \"page\", \"col l/r\", \"table from top\", \"entry nr\"]\n",
        "counter = df[subset].isna().any(axis=1).sum()\n",
        "print(f\"A total of {counter} rows contained NaNs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcRQYEBSkLGV",
        "outputId": "b59e61af-032b-4739-db03-02995db9589b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "containing NaNs:\n",
            "A total of 10 rows contained NaNs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## QC\n",
        "\n",
        "## Checking for duplicates\n",
        "## Replaced reference with reference_code as it is the real column name\n",
        "## Not actually needed anymore due ot duplicable table column\n",
        "\n",
        "if True:\n",
        "    ## duplicates and errors\n",
        "    test_df = df\n",
        "    MANUALLY_EXCLUDED_DUPLICATES = [\n",
        "        \"54STA\",\n",
        "        \"71TAN/JOH\",\n",
        "        \"91HOR/UEH\",\n",
        "        \"76SCH/KRI\",\n",
        "        \"99TEW/SCH\"\n",
        "    ]\n",
        "    ## shouldn't be necessary after introduction of duplicate_table column; so we use:\n",
        "    MANUALLY_EXCLUDED_DUPLICATES = []\n",
        "    ##\n",
        "    test_df = test_df[~test_df.reference_code.isin(MANUALLY_EXCLUDED_DUPLICATES)]\n",
        "    assert sum(test_df[((test_df[\"entry nr\"]==\"duplicate\") | (test_df[\"entry nr\"]==\"error\"))].id.isna())==0, (\"Duplicate or error found for an empty-ID row\", test_df[(((test_df[\"entry nr\"]==\"duplicate\") | (test_df[\"entry nr\"]==\"error\"))) & test_df.id.isna()])\n",
        "    ## Ensures that rows containing duplicate or error in entry nr column also has an appropriate ID.\n",
        "\n",
        "#print(\"I am removing the following duplicates and errors:\")\n",
        "df = df[~((df[\"entry nr\"]==\"duplicate\") | (df[\"entry nr\"]==\"error\"))]\n",
        "## Removes such rows"
      ],
      "metadata": {
        "id": "qkoNWXhx_LQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QC\n",
        "if True:\n",
        "    ##check completeness of position annotation\n",
        "    na_counter = df[[\"part\",\"page\",\"col l/r\",\"table from top\", \"entry nr\"]].isna().sum(axis=\"columns\") ##Counts NaN values in position annotation\n",
        "    #print(na_counter)\n",
        "    assert len(df[~na_counter.isin([0,5])])==0, print(df[~na_counter.isin([0,5])][[\"id\",\"reference\",\"part\",\"page\",\"col l/r\",\"table from top\", \"entry nr\"]].to_string())\n",
        "    #Prints problematic rows if any\n",
        "\n",
        "    ##Checks for spaces in /reference_code/ column.\n",
        "    assert len(df[df.reference_code.str.contains(\" \").fillna(False)])==0, print(df[df.reference_code.str.contains(\" \").fillna(False)].to_string())\n",
        "    ## And output problematic rows.\n",
        "\n",
        "## drop NaNs -- these entries just haven't been worked on and can't be checked\n",
        "df = df.dropna(subset=[\"part\",\"page\",\"col l/r\",\"table from top\", \"entry nr\"])\n",
        "\n",
        "## convert values in columns part, page, col l/r, table from top and entry nr to integers\n",
        "df[[\"part\",\"page\",\"col l/r\",\"table from top\", \"entry nr\"]] = df[[\"part\",\"page\",\"col l/r\",\"table from top\", \"entry nr\"]].astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwqHEVvMC3OC",
        "outputId": "03d7517d-d10a-4226-a166-164f1e50ab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-3d14777ca5e3>:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  assert len(df[df.reference_code.str.contains(\" \").fillna(False)])==0, print(df[df.reference_code.str.contains(\" \").fillna(False)].to_string())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## quality check new data\n",
        "\n",
        "if True:\n",
        "    ## id is unique\n",
        "    assert len(df.dropna(subset=[\"id\"]).id.unique()) == len(df.dropna(subset=[\"id\"])), df.dropna(subset=[\"id\"])[df.dropna(subset=[\"id\"]).id.duplicated()].to_string()\n",
        "    ## Ensures that every value in column ID is unique, if not prints out problematic tows.\n",
        "\n",
        "    ## tables intact in themselves\n",
        "\n",
        "    ##Groups rows in dataframe by part, page, col l/r, and table from top values.\n",
        "\n",
        "    for which, g in df.groupby([\"part\",\"page\",\"col l/r\",\"table from top\"]): # g is subset corresponding to specific part of the dataframe, as grouped by previously\n",
        "        #print((which,g))\n",
        "        #if which == (3, 1091, 1, 1):\n",
        "        #    continue\n",
        "\n",
        "        assert len(g.reference_code.unique())==1, (which, print(g.to_string())) ## Checks that all values in reference_code have the same value--\n",
        "        assert len(g.EC.unique()) == 1, (which, print(g.to_string())) ## Check that all EC values are uniform\n",
        "        assert len(g.reaction.unique())==1, (which, print(g.to_string())) ## - fixed \"description\" for correct column name\n",
        "        assert sorted(g[\"entry nr\"].values)==list(range(1,g[\"entry nr\"].max()+1)), (which, print(g.to_string())) ## Check that all values in entry nr are ordered from lowest to highest\n",
        "\n",
        "    ## table counts consistently continuous\n",
        "    for which, g in df.groupby([\"part\", \"page\", \"col l/r\"]):\n",
        "        ## the following part/page/column combinations\n",
        "        #\n",
        "        # contain a table which was mentioned before;\n",
        "        ## the table is thus marked full duplicate and was removed beforehand; so we manually\n",
        "        ## exclude those from this automated check\n",
        "        MANUALLY_EXCLUDED_COLUMNS = [\n",
        "            (2, 558, 2),\n",
        "            (2, 560, 2),\n",
        "            (2, 566, 2),\n",
        "            (2, 584, 1),\n",
        "            (2, 590, 2),\n",
        "            (3, 1041, 1),\n",
        "            (3, 1076, 2),\n",
        "            (7, 1360, 2),\n",
        "            (7, 1369, 2),\n",
        "            (7, 1373, 1),\n",
        "        ]\n",
        "        ## shouldn't be necessary after introduction of duplicate_table column; so we use:\n",
        "        MANUALLY_EXCLUDED_COLUMNS = []\n",
        "        ## because there is a peculiarity about tables not-existent-in-the-pdf, but found in randr, we have to use:\n",
        "        MANUALLY_EXCLUDED_COLUMNS = [\n",
        "            (6, 948, 2),\n",
        "            (6, 949, 1),\n",
        "        ]\n",
        "        if which in MANUALLY_EXCLUDED_COLUMNS:\n",
        "            continue\n",
        "        assert sorted(g[\"table from top\"].unique()) == list(range(1, g[\"table from top\"].max() + 1)), (which, print(g.to_string())) # ensures that values in table from top are from lowest to highest\n",
        "\n",
        "    ## column values either 1 or 2\n",
        "    for which, g in df.groupby([\"part\", \"page\"]):\n",
        "        assert all([i in [1,2] for i in g[\"col l/r\"].values]), (which, print(g.to_string())) # ensures that all values in col l/r are either 1 or 2\n",
        "\n",
        "    ## page numbers consistently continuous\n",
        "    for which, g in df.groupby(\"part\"):\n",
        "        MANUALLY_EXCLUDED_PARTS = [\n",
        "            #2,\n",
        "            #3,\n",
        "            #7\n",
        "        ]\n",
        "        if which in MANUALLY_EXCLUDED_PARTS: #ensures that page numbers are continious by parts\n",
        "            continue\n",
        "\n",
        "        print(f\"----- This is about part {which} -----\")\n",
        "        should_be = list(range(g[\"page\"].min(), g[\"page\"].max() + 1))\n",
        "        for page in should_be:\n",
        "            if page not in g[\"page\"].unique():\n",
        "                print(f\"Missing page: {page}\") #Finds missing pages and prints them out\n",
        "        #assert sorted(g[\"page\"].unique()) == list(range(g[\"page\"].min(), g[\"page\"].max() + 1))\n",
        "\n",
        "## extract added values\n",
        "# print(f\"You will need to care for these {df.id.isna().sum()} recently added rows:\")\n",
        "# print(df[df.id.isna()])\n",
        "\n",
        "print(\"The online spreadsheet data looks consistent.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUlWBY4aI8ri",
        "outputId": "615d1079-f8ef-4e0a-cc78-30f0f2a4ff50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- This is about part 1 -----\n",
            "----- This is about part 2 -----\n",
            "----- This is about part 3 -----\n",
            "Missing page: 1088\n",
            "Missing page: 1089\n",
            "Missing page: 1090\n",
            "----- This is about part 4 -----\n",
            "----- This is about part 5 -----\n",
            "----- This is about part 6 -----\n",
            "----- This is about part 7 -----\n",
            "The online spreadsheet data looks consistent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## consistency check between online spreadsheet and original Noor data\n",
        "##Reading TECRDB and openTECR files\n",
        "noor = pandas.read_csv(\"TECRDB.csv\")\n",
        "noor = noor.rename(columns={\"reaction\": \"keggID\"})\n",
        "noor = noor.rename(columns={\"reference\": \"reference_code\",'description':'reaction'})\n",
        "## Updating column names in Noor set to match the new columns in openTECR.\n",
        "if READ_CSV:\n",
        "    online = pandas.read_csv(\"openTECR recuration - actual data.csv\")\n",
        "else:\n",
        "    online = pandas.read_excel(\"openTECR recuration.ods\", sheet_name=\"actual data\")\n",
        "\n",
        "\n",
        "online = online.replace({\"col l/r\": {\"l\":1,\"r\":2}}) #replacing l/r values with 1 and 2\n",
        "## check that all ids are still there\n",
        "assert set(noor.id) - set(online.id) == set(), f\"The following IDs were deleted online: {set(noor.id)-set(online.id)}\"\n",
        "## Checks that all ID are present in both openTECR and Noor\n",
        "\n",
        "## non-curated values should not have been changed by anyone!\n",
        "leftjoined = pandas.merge(noor, online.dropna(subset=\"id\"), on=\"id\", how=\"left\", validate=\"1:1\")\n",
        "## Merging noor set and openTECR together"
      ],
      "metadata": {
        "id": "qogRI2yEodFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This code snippet tries to check if K_prime values are equal in Noor and openTECR dataset\n",
        "## While accounting for rounding up in openTECR\n",
        "## If values are not equal despite being rounded up, it will output both k_prime values from noor and openTECR\n",
        "## kPn is a prime_K column from Noor set roughly rounded up by the rules outlined in openTECR\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "kPn = []\n",
        "for i in range(len(leftjoined.K_prime_x)):\n",
        "  if leftjoined.K_prime_x[i] != leftjoined.K_prime_y[i]:\n",
        "    if np.isnan(leftjoined.K_prime_x[i]) == False:\n",
        "      #print(leftjoined.K_prime_x[i])\n",
        "      #print(leftjoined.K_prime_y[i])\n",
        "      ##Trying to match weird rounding up format of openTECR Kprime values\n",
        "      if len(str(leftjoined.K_prime_x[i]).split('.')[1])>4:\n",
        "        rounded = round(leftjoined.K_prime_x[i],len(str(leftjoined.K_prime_x[i]).split('.')[1])-1)\n",
        "        #print(rounded)\n",
        "      elif len(str(leftjoined.K_prime_x[i]).split('.')[1])==2:\n",
        "        rounded = round(leftjoined.K_prime_x[i],1)\n",
        "        #print(rounded)\n",
        "      elif len(str(leftjoined.K_prime_x[i]).split('.')[1])==3:\n",
        "        rounded = round(leftjoined.K_prime_x[i], 2)\n",
        "        #print(rounded)\n",
        "      elif len(str(leftjoined.K_prime_x[i]).split('.')[1])==1:\n",
        "        rounded = round(leftjoined.K_prime_x[i], 0)\n",
        "        #print(rounded)\n",
        "      #print(round(leftjoined.K_prime_x[i], 2))\n",
        "      if rounded+0.1 == leftjoined.K_prime_y[i]:\n",
        "        kPn.append(rounded+0.1)\n",
        "      else:\n",
        "        kPn.append(rounded)\n",
        "    else:\n",
        "      kPn.append(leftjoined.K_prime_x[i])\n",
        "  else:\n",
        "    kPn.append(leftjoined.K_prime_x[i])\n",
        "for i in range(len(leftjoined.K_prime_y)):\n",
        "  if kPn[i] != leftjoined.K_prime_y[i]:\n",
        "    if np.isnan(leftjoined.K_prime_y[i]) == False:\n",
        "      print(f'Noor set K_prime value: {kPn[i]}')\n",
        "      print(f'openTECR set K_prime value: {leftjoined.K_prime_y[i]}')\n",
        "## Some cases couldn't be handled since they don't follow rounding logic"
      ],
      "metadata": {
        "id": "SXZzIqZZohFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Columns that should be the same\n",
        "SHOULD_BE_THE_SAME = [\n",
        "    \"reference_code\",\n",
        "    \"EC\",\n",
        "    \"reaction\",\n",
        "    \"K\",\n",
        "    \"temperature\",\n",
        "    \"ionic_strength\",\n",
        "    \"p_h\",\n",
        "    \"p_mg\",\n",
        "    #\"K_prime\", # - checked manually, weird rounding up in online data? Handled by code above\n",
        "]\n",
        "for s in SHOULD_BE_THE_SAME:\n",
        "    entries_where_both_are_nans = leftjoined[ leftjoined[f\"{s}_x\"].isna() & leftjoined[f\"{s}_y\"].isna() ]\n",
        "    if len(entries_where_both_are_nans) == 0:\n",
        "        assert (leftjoined[f\"{s}_x\"] == leftjoined[f\"{s}_y\"]).all(), (s, print(leftjoined[~(leftjoined[f\"{s}_x\"] == leftjoined[f\"{s}_y\"])][[\"id\",f\"{s}_x\",f\"{s}_y\"]].to_string()))\n",
        "    else:\n",
        "        tmp = leftjoined[ ~ (leftjoined[f\"{s}_x\"].isna() & leftjoined[f\"{s}_y\"].isna()) ]\n",
        "        assert (tmp[f\"{s}_x\"] == tmp[f\"{s}_y\"]).all(), (s, print(tmp[~(tmp[f\"{s}_x\"] == tmp[f\"{s}_y\"])][[\"id\",f\"{s}_x\",f\"{s}_y\"]].to_string()))\n",
        "\n",
        "## The code above checks that all values in those columns between both Noor (_x) and openTECR (_y) are either NaN or equal. If not, raises assertion error and prints out incorrect rows\n",
        "\n",
        "## did someone add a new row without id, where they should have corrected a row with id?\n",
        "merged = pandas.merge(online, noor, on=[\n",
        "    \"reference_code\",\n",
        "    \"temperature\",\n",
        "    \"ionic_strength\",\n",
        "    \"p_h\",\n",
        "    \"p_mg\",\n",
        "    \"K_prime\",\n",
        "])\n",
        "merged = merged[merged[\"id_x\"] != merged[\"id_y\"]]\n",
        "potential_errors = merged[merged[\"id_x\"].isna() | merged[\"id_y\"].isna()]\n",
        "\n",
        "## Checks whether or not noor and online set have matching ID values\n",
        "\n",
        "## the following have been manually checked to be able to be excluded from the comparison below:\n",
        "MANUALLY_EXCLUDED = [\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4356\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1714\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1715\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1716\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1718\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1717\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2140\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2149\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3167\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3184\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry707\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4246\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry392\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1269\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4283\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4284\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3888\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3896\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3897\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1915\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1603\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1605\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry236\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry386\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2718\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3560\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3561\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4006\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1271\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1586\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1589\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1590\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1591\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1588\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1587\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2370\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2371\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1601\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3637\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2725\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry705\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry702\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry704\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4040\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4373\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1829\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3031\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3032\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2339\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry1916\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3026\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4086\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2848\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry4316\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry772\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2851\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry398\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry82\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry806\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry805\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry3654\",\n",
        "    \"https://w3id.org/related-to/doi.org/10.5281/zenodo.3978439/files/TECRDB.csv#entry2277\",\n",
        "]\n",
        "potential_errors = potential_errors[ ~potential_errors.id_y.isin(MANUALLY_EXCLUDED) ]\n",
        "if len(potential_errors) > 0:\n",
        "    print(\"The following entries might have been added as a new row without id, where they should have corrected a row with id:\")\n",
        "    print(potential_errors.to_string())\n",
        "\n",
        "print(\"The online spreadsheet data and its original data source are still in sync.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-RFQB1zgc2N",
        "outputId": "17acfa5d-6058-4928-e7d1-5e4069772ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The online spreadsheet data and its original data source are still in sync.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## annotate online spreadsheet with table_codes\n",
        "#\n",
        "#\n",
        "## read second df\n",
        "## -- get it manually from https://zenodo.org/record/5495826 !\n",
        "noor = pandas.read_csv(\"TECRDB.csv\")\n",
        "noor.columns\n",
        "#Index(['id', 'url', 'reference', 'method', 'eval', 'EC', 'enzyme_name',\n",
        "#       'reaction', 'description', 'K', 'K_prime', 'temperature',\n",
        "#       'ionic_strength', 'p_h', 'p_mg'],\n",
        "\n",
        "noor = noor.drop(['K', 'K_prime', 'temperature',\n",
        "       'ionic_strength', 'p_h', 'p_mg'],axis=\"columns\")\n",
        "\n",
        "## Fixed to matching name in openTECR\n",
        "noor = noor.rename(columns={\"reaction\": \"keggID\"})\n",
        "noor = noor.rename(columns={\"reference\": \"reference_code\",'description':'reaction'})\n",
        "\n",
        "## extract table codes\n",
        "noor[\"table_code\"] = noor.url.str.split(\"&T1=\").str[-1]\n",
        "\n",
        "## quality check\n",
        "if True:\n",
        "    ## tables intact in themselves\n",
        "    for which, g in noor.groupby(\"table_code\"):\n",
        "        #print((which,g))\n",
        "        assert len(g.reference_code.unique())==1, (which, print(g.to_string()))\n",
        "        assert len(g.method.unique())==1, (which, print(g.to_string()))\n",
        "        assert len(g[\"eval\"].unique()) == 1, (which, print(g.to_string()))\n",
        "        assert len(g.EC.unique()) == 1, (which, print(g.to_string()))\n",
        "        assert len(g.enzyme_name.unique())==1, (which, print(g.to_string()))\n",
        "        assert len(g.reaction.unique()) == 1, (which, print(g.to_string()))\n",
        "        assert len(g.reaction.unique()) == 1, (which, print(g.to_string()))\n",
        "\n",
        "## Ensures that all values in those columns are consistent\n",
        "\n",
        "## drop now-unnecessary columns\n",
        "noor = noor.drop([\"EC\",\"reference_code\", \"reaction\"], axis=\"columns\")\n",
        "#df   = df.drop(  [\"description\"],    axis=\"columns\")\n",
        "\n",
        "## merge\n",
        "tmp = pandas.merge(df, noor, how=\"left\", on=\"id\")\n",
        "\n",
        "## add manually extracted table codes\n",
        "if READ_CSV:\n",
        "    manual_table_codes = pandas.read_csv(\"openTECR recuration - table codes.csv\")\n",
        "else:\n",
        "    manual_table_codes = pandas.read_excel(\"openTECR recuration.ods\", sheet_name=\"manually mapped table codes\")\n",
        "\n",
        "# QC\n",
        "if True:\n",
        "    assert sum(manual_table_codes.duplicated([\"part\", \"page\", \"col l/r\", \"table from top\"])) == 0, print(\n",
        "        manual_table_codes[manual_table_codes.duplicated([\"part\", \"page\", \"col l/r\", \"table from top\"])])\n",
        "# split into tables with table codes from Noor and those which needed to be annotated manually\n",
        "manual_table_codes = manual_table_codes.drop([\"reference\", \"description\"], axis=\"columns\")\n",
        "tmp_with_table_codes = tmp[~tmp.table_code.isna()]\n",
        "tmp_without_table_codes = tmp[tmp.table_code.isna()]\n",
        "tmp_without_table_codes = tmp_without_table_codes.drop(\"table_code\", axis=\"columns\")\n",
        "tmp_without_table_codes_try_to_add_manual_ones = pandas.merge(tmp_without_table_codes, manual_table_codes, how=\"left\", on=[\"part\",\"page\",\"col l/r\",\"table from top\"])\n",
        "# concat the two\n",
        "new = pandas.concat([tmp_with_table_codes, tmp_without_table_codes_try_to_add_manual_ones], ignore_index=True)\n",
        "## keep only one entry per table code, remove now-meaningless columns, but keep id=NaN rows\n",
        "new = new[~new.duplicated([\"part\",\"page\",\"col l/r\",\"table from top\"])]\n",
        "new = new.drop([\"id\",\"url\"], axis=\"columns\")\n",
        "new[\"comment\"] = \"\"\n",
        "\n",
        "## Ensures no diplicate values in manual_table_codes\n",
        "## Splits tmp into portitions with and without table_codes\n",
        "## Removes duplicate entries and adds column \"comment\" for further annotation\n",
        "\n",
        "## export tables which need to be added to the \"table codes\" tab\n",
        "selector = []\n",
        "for i,s in new.iterrows():\n",
        "    if pandas.isna(s.table_code):\n",
        "        if len(manual_table_codes[(manual_table_codes.part == s.part) &\n",
        "                            (manual_table_codes.page == s.page) &\n",
        "                            (manual_table_codes[\"col l/r\"] == s[\"col l/r\"]) &\n",
        "                            (manual_table_codes[\"table from top\"] == s[\"table from top\"])\n",
        "           ]) == 0:\n",
        "            selector.append(i)\n",
        "export = new.loc[selector]\n",
        "(export[[\n",
        "    \"part\",\"page\",\"col l/r\",\"table from top\",\n",
        "    \"table_code\",\n",
        "    \"reaction\",\n",
        "    \"reference_code\",\n",
        "    \"curator\",\n",
        "    ]]\n",
        " .sort_values([\"part\",\"page\",\"col l/r\",\"table from top\"])\n",
        " .to_csv(\"2024-01-06-opentecr-recuration.missing_table_codes.csv\", index=False)\n",
        " )\n",
        "\n",
        "## identifies and exports rows from the new DataFrame that are missing table_code values and do not already exist in manual_table_codes, creating a CSV file with them\n",
        "## selector - rows that need to be exported\n",
        "\n",
        "\n",
        "## export tables which need to have their comment extracted\n",
        "selector = []\n",
        "if READ_CSV:\n",
        "    tables_with_comments = pandas.read_csv(\"openTECR recuration - table metadata.csv\")\n",
        "else:\n",
        "    tables_with_comments = pandas.read_excel(\"openTECR recuration.ods\", sheet_name=\"table comments\")\n",
        "## QC\n",
        "if True:\n",
        "    assert sum(tables_with_comments.duplicated([\"part\",\"page\",\"col l/r\",\"table from top\"]))==0, print(tables_with_comments[tables_with_comments.duplicated([\"part\",\"page\",\"col l/r\",\"table from top\"])])\n",
        "\n",
        "# Identify rows in new df that are not present in the metadata\n",
        "for i, s in new.iterrows():\n",
        "    # Check if a matching entry exists in `tables_with_comments`\n",
        "    if len(tables_with_comments[\n",
        "        (tables_with_comments.part == s.part) &\n",
        "        (tables_with_comments.page == s.page) &\n",
        "        (tables_with_comments[\"col l/r\"] == s[\"col l/r\"]) &\n",
        "        (tables_with_comments[\"table from top\"] == s[\"table from top\"])\n",
        "    ]) == 0:\n",
        "        # If no match is found, add the index of the row to `selector`\n",
        "        selector.append(i)\n",
        "\n",
        "tables_without_comments = new.loc[selector]\n",
        "# Add placeholders for manual annotation and spellchecking\n",
        "tables_without_comments[\"manually spellchecked\"] = \"\"\n",
        "tables_without_comments[\"comment\"] = \"\"\n",
        "\n",
        "\n",
        "# Export the filtered data to a CSV file\n",
        "\n",
        "(tables_without_comments[[\n",
        "    \"part\",\"page\",\"col l/r\",\"table from top\",\n",
        "    \"reference_code\",\n",
        "    \"manually spellchecked\",\n",
        "    \"comment\"\n",
        "    ]]\n",
        " .sort_values([\"part\",\"page\",\"col l/r\",\"table from top\"])\n",
        " .to_csv(\"2024-01-06-opentecr-recuration.missing-table-comments.csv\", index=False)\n",
        " )\n",
        "\n",
        "print(\"I could merge the online spreadsheet and the Noor data. I have written file regarding the tables to your disk.\")"
      ],
      "metadata": {
        "id": "9pjJJF2TX0rl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}